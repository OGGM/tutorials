{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4e5293",
   "metadata": {},
   "source": [
    "# Merge, analyse and visualize OGGM GCM runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79fd02",
   "metadata": {},
   "source": [
    "In this notebook we want to show: \n",
    "- How to merge the output of different OGGM GCM projections into one dataset\n",
    "- How to calculate regional values (e.g. volume) using the merged dataset\n",
    "- How to use [HoloViews](https://holoviews.org/) and [Panel](https://panel.holoviz.org/) to visualize the outcome. This part uses advanced plotting capabilities which are not necessary to understand the rest of the notebook.\n",
    "\n",
    "This notebook is intended to explain the postprocessing steps, rather than the OGGM workflow itself. Therefore some code (especially conducting the GCM projection runs) does not have many explanations. If you are more interested in these steps you should check out the notebook [Run OGGM with GCM data](run_with_gcm.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d387c",
   "metadata": {},
   "source": [
    "## GCM projection runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fdf37",
   "metadata": {},
   "source": [
    "The first step is to conduct the GCM projection runs. We choose two different glaciers by their rgi_ids and conduct the GCM projections. Again if you do not understand all of the following code you should check out the [Run OGGM with GCM data](run_with_gcm.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "from time import gmtime, strftime\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as shpg\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "\n",
    "hv.extension('bokeh')\n",
    "pn.extension()\n",
    "\n",
    "# Locals\n",
    "from oggm import utils, workflow, tasks\n",
    "import oggm.cfg as cfg\n",
    "from oggm.shop import gcm_climate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c0a56",
   "metadata": {},
   "source": [
    "### Pre-processed directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OGGM and set up the default run parameters\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "\n",
    "# change border around the individual glaciers\n",
    "cfg.PARAMS['border'] = 40\n",
    "\n",
    "# Use Multiprocessing\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "\n",
    "# For hydro output\n",
    "cfg.PARAMS['store_model_geometry'] = True\n",
    "\n",
    "# Local working directory (where OGGM will write its output)\n",
    "cfg.PATHS['working_dir'] = utils.gettempdir('OGGM_merge_gcm_runs', reset=True)\n",
    "\n",
    "# RGI glaciers: Ngojumba and Khumbu\n",
    "rgi_ids = utils.get_rgi_glacier_entities(['RGI60-15.03473', 'RGI60-15.03733'])\n",
    "\n",
    "# Go - get the pre-processed glacier directories\n",
    "gdirs = workflow.init_glacier_directories(rgi_ids, from_prepro_level=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3475a8",
   "metadata": {},
   "source": [
    "### Download and process GCM data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56314005",
   "metadata": {},
   "source": [
    "Here we define all the CMIP5 GCM models and the RCP scenarios we want to use. [Here](https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/gcm_table.html) you can get an overview of all available options. As you see in the overview only for two of the three GCMs the rcp60 Scenario is available! We deal with this issue by including a ```try```/```except``` in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b141220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the GCM models you want to use\n",
    "all_GCM = ['CCSM4',\n",
    "           'CNRM-CM5',\n",
    "           'CSIRO-Mk3-6-0',\n",
    "          ]\n",
    "\n",
    "# define the rcp scenarios to use\n",
    "all_rcp = ['rcp26',\n",
    "           'rcp45',\n",
    "           'rcp60',\n",
    "           'rcp85']\n",
    "\n",
    "# download locations for precipitation and temperature\n",
    "bp = 'https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/pr/pr_mon_{}_{}_r1i1p1_g025.nc'\n",
    "bt = 'https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/tas/tas_mon_{}_{}_r1i1p1_g025.nc'\n",
    "\n",
    "for GCM in all_GCM:\n",
    "    for rcp in all_rcp:\n",
    "        # Download the files\n",
    "        ft = utils.file_downloader(bt.format(GCM, rcp))\n",
    "        fp = utils.file_downloader(bp.format(GCM, rcp))\n",
    "\n",
    "        try:\n",
    "            # bias correct them\n",
    "            workflow.execute_entity_task(gcm_climate.process_cmip_data, gdirs, \n",
    "                                         filesuffix='_{}_{}'.format(GCM, rcp),  # recognize the climate file for later\n",
    "                                         fpath_temp=ft,  # temperature projections\n",
    "                                         fpath_precip=fp,  # precip projections\n",
    "                                         );\n",
    "        except ValueError:\n",
    "            # if a certain scenario is not available for a GCM we land here\n",
    "            # and we inidcate this by printing a message so the user knows\n",
    "            # this scenario is missing\n",
    "            print('No ' + GCM +' run with scenario ' + rcp + ' available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c8587",
   "metadata": {},
   "source": [
    "### Actual projection Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b079e",
   "metadata": {},
   "source": [
    "Now we conduct the actual projection Runs. Again handling the case that for a certain (GCM, RCP) combination no data is available with a ```try```/```except```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29abdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for GCM in all_GCM:\n",
    "    for rcp in all_rcp:\n",
    "        rid = '_{}_{}'.format(GCM, rcp)\n",
    "        try:  # check if GCM, RCP combination exists\n",
    "            workflow.execute_entity_task(tasks.run_with_hydro, gdirs,\n",
    "                                         run_task=tasks.run_from_climate_data,\n",
    "                                         ys=2020,  # star year of our projection runs\n",
    "                                         climate_filename='gcm_data',  # use gcm_data, not climate_historical\n",
    "                                         climate_input_filesuffix=rid,  # use the chosen GCM and scenario\n",
    "                                         init_model_filesuffix='_historical',  # this is important! Start from 2020 glacier\n",
    "                                         output_filesuffix=rid,  # the filesuffix of the resulting file, so we can find it later\n",
    "                                         store_monthly_hydro=True\n",
    "                                        );\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            # if a certain scenario is not available for a GCM we land here\n",
    "            # and we inidcate this by printing a message so the user knows\n",
    "            # this scenario is missing\n",
    "            print('No ' + GCM +' run with scenario ' + rcp + ' available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12feb23a",
   "metadata": {},
   "source": [
    "## Merge datasets together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f791e8c",
   "metadata": {},
   "source": [
    "Now that we have conducted the projection Runs we can start merging all outputs into one dataset. The individual files can be opened by providing the ```output_filesuffix``` of the projection runs as the ```input_filesuffix```. Let us have a look at one resulting dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id  = '_{}_{}'.format(all_GCM[0], all_rcp[0])\n",
    "ds = utils.compile_run_output(gdirs, input_filesuffix=file_id)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a29a8",
   "metadata": {},
   "source": [
    "We see that as a result, we get a [xarray.Dataset](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.html). We also see that this dataset has three dimensions ```time```, ```rgi_id``` and ```month_2d```. When we look at the ```Data variables``` (click to expand) we can see that for each of these variables a certain combination of the dimensions is given. For example, for the ```volume``` we see the dimensions ```(time, rgi_id)``` which indicates that the underlying data is separated by each year and each glacier. Let's have a look at the volume in 2030 and 2040 at the glacier 'RGI60-15.03473' (Ngojumba):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['volume'].loc[{'time': [2030, 2040], 'rgi_id': ['RGI60-15.03473' ]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d18aa6",
   "metadata": {},
   "source": [
    "A more complete overview of how to access the data of xarray Dataset can be found [here](https://xarray.pydata.org/en/stable/user-guide/indexing.html#dataset-indexing).\n",
    "\n",
    "This is quite useful, but unfortunately, we have one xarray Dataset for each (GCM, RCP) pair. For an easier calculation and comparison between different GCMs and RCPs, it would be desirable to combine all individual Datasets into one Dataset with the additional dimensions ```(GCM, RCP)```. Fortunately, xarray provides different functions to merge Datasets ([here](https://xarray.pydata.org/en/stable/user-guide/combining.html) you can find an Overview).\n",
    "\n",
    "In our case, we want to merge all individual datasets with two additional coordinates ```GCM``` and ```RCP```. Additional coordinates can be added with the comment ```.coords[]```, also you can include a description with ```.coords[].attrs['description']```. For example, let us add ```GCM``` as a new coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5dbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coords['GCM'] = all_GCM[0]\n",
    "ds.coords['GCM'].attrs['description'] = 'Global Circulation Model'\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67effd31",
   "metadata": {},
   "source": [
    "You can see that now ```GCM``` is added as Coordinate, but when you look at the Data variables you see that this new Coordinate is not used by any of them. So we must add this new Coordinate to the Data variables using the ```.expand_dims()``` comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33361f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.expand_dims('GCM')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052351e",
   "metadata": {},
   "source": [
    "Now we see that ```GCM``` was added to the Dimensions and all Data variables now use this new coordinate. The same can be done with ```RCP```. As a standalone, this is not very useful. But if we add these coordinates to all datasets, it becomes quite handy for merging. Therefore we now open all datasets and add to each one the two coordinates ```GCM``` and ```RCP```:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e7302",
   "metadata": {},
   "source": [
    "### Add new coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = []  # in this array all datasets going to be stored with additional coordinates GCM and RCP\n",
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "for GCM in all_GCM:  # loop through all GCMs\n",
    "    for RCP in all_rcp:  # loop through all RCPs\n",
    "        try:  # check if GCM, RCP combination exists\n",
    "            rid = '_{}_{}'.format(GCM, RCP)  # put together the same filesuffix which was used during the projection runs\n",
    "\n",
    "            ds_tmp = utils.compile_run_output(gdirs, input_filesuffix=rid)  # open one model run\n",
    "\n",
    "            ds_tmp.coords['GCM'] = GCM  # add GCM as a coordinate\n",
    "            ds_tmp.coords['GCM'].attrs['description'] = 'used Global circulation Model'  # add a description for GCM\n",
    "            ds_tmp = ds_tmp.expand_dims(\"GCM\")  # add GCM as a dimension to all Data variables\n",
    "\n",
    "            ds_tmp.coords['RCP'] = RCP  # add RCP as a coordinate\n",
    "            ds_tmp.coords['RCP'].attrs['description'] = 'used Representative Concentration Pathway'  # add a description for RCP\n",
    "            ds_tmp = ds_tmp.expand_dims(\"RCP\")  # add RCP as a dimension to all Data variables\n",
    "\n",
    "            ds_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "            ds_all.append(ds_tmp)  # add the dataset with extra coordinates to our final ds_all array\n",
    "\n",
    "        except RuntimeError as err:  # here we land if an error occured\n",
    "            if str(err) == 'Found no valid glaciers!':  # This is the error message if the GCM, RCP combination does not exist\n",
    "                print(f'No data for GCM {GCM} with RCP {RCP} found!')  # print a descriptive message\n",
    "            else:\n",
    "                raise RuntimeError(err)  # if an other error occured we just raise it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d149089",
   "metadata": {},
   "source": [
    "### Actual merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4a1b4",
   "metadata": {},
   "source": [
    "```ds_all``` now contains all outputs with the additionally added coordinates. Now we can merge them with the very convenient xarray function ```xr.combine_by_coords()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merged = xr.combine_by_coords(ds_all,\n",
    "                                 fill_value=np.nan)  # define how the missing GCM, RCP combinations should be filled\n",
    "ds_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5a4f6",
   "metadata": {},
   "source": [
    "Great! You see that the result is one Dataset, but we still can identify the individual runs with the two additionally added Coordinates GCM and RCP. If you want to save this new Dataset for later analysis you can use for example ```.to_netcdf()``` ([here](https://xarray.pydata.org/en/stable/user-guide/io.html) you can find an Overview of xarray writing options):\n",
    "\n",
    "```python\n",
    "ds_merged.to_netcdf('merged_GCM_projections.nc')\n",
    "```\n",
    "\n",
    "And to open it again later and automatically close the file after reading you can use:\n",
    "\n",
    "```python\n",
    "with xr.open_dataset('merged_GCM_projections.nc') as ds:\n",
    "    ds_merged = ds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a0539",
   "metadata": {},
   "source": [
    "## Calculate regional values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd816e6",
   "metadata": {},
   "source": [
    "Now we have our merged dataset and we can start with the analysis. To do this xarray gives us a ton of different mathematical functions which can be used directly with our dataset. [Here](https://xarray.pydata.org/en/stable/user-guide/computation.html) you can find a great overview of what is possible.\n",
    "\n",
    "As an example, we want to calculate the mean and std of the total regional glacier volume for different scenarios of our merged dataset. Here our added coordinates come in very handy.\n",
    "\n",
    "Let us start by calculating the total regional glacier volume using ```.sum()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_total_volume = ds_merged['volume'].sum(dim='rgi_id',  # over which dimension the sum should be taken, here we want to sum up over all glacier ids\n",
    "                                          skipna=True,  # ignore nan values\n",
    "                                          keep_attrs=True)  # keep the variable descriptions\n",
    "ds_total_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7cfcf",
   "metadata": {},
   "source": [
    "For this calculation, we told xarray over which dimension we want to sum up ```dim='rgi_id'``` (all glaciers ), that missing values should be ignored ```skipna=True``` (as some GCM/RCP combinations are not available) and that the attributes (the description of the variables) should be preserved ```keep_attrs=True```. You can see that the result has three dimensions left ```(GCM, time, RCP)``` and the dimension ```rgi_id``` disappeared as we have summed all values up along this dimension.\n",
    "\n",
    "Likewise, we can now calculate the mean of all GCM runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf62bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_total_volume_mean = ds_total_volume.mean(dim='GCM',  # over which dimension the mean should be calculated\n",
    "                                            skipna=True,  # ignore nan values\n",
    "                                            keep_attrs=True)  # keep all variable descriptions\n",
    "ds_total_volume_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237d48e",
   "metadata": {},
   "source": [
    "And you see that now we are left with two dimensions ```(RCP, time)```. This means we now have calculated the mean total volume for all different RCP scenarios and along the projection period. The standard deviation (std) can be also calculated in the same way as the mean.\n",
    "\n",
    "If you want to calculate the total mean and std values for different variables it is convenient to define a small function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6606ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_mean_and_std(ds, variable):\n",
    "    mean = ds[variable].sum(dim='rgi_id',  # first sum up over all glaciers\n",
    "                            skipna=True,\n",
    "                            keep_attrs=True,\n",
    "                           ).mean(dim='GCM',  # afterwards calculate the mean of all GCMs\n",
    "                                  skipna=True,\n",
    "                                  keep_attrs=True,\n",
    "                                 )\n",
    "    std = ds[variable].sum(dim='rgi_id',  # first sum up over all glaciers \n",
    "                           skipna=True,\n",
    "                           keep_attrs=True,\n",
    "                          ).std(dim='GCM',  # afterwards calculate the std of all GCMs\n",
    "                                skipna=True,\n",
    "                                keep_attrs=True,\n",
    "                               )\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19285f08",
   "metadata": {},
   "source": [
    "This function takes a dataset ```ds``` and a variable name ```variable``` for which the mean and the std should be calculated. This function will come in handy in the next section dealing with the visualisation of the calculated values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a10d69",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f8d1c",
   "metadata": {},
   "source": [
    "Now we can calculate the regional values of our projection runs it is time to visualize them. In the following, we show one way to visualize the data using tools from the [HoloViz](https://holoviz.org/) framework (namely [HoloViews](https://holoviews.org/) and [Panel](https://panel.holoviz.org/)). For an introduction to HoloViz, you can have a look at the [Small overview of HoloViz capability of data exploration](holoviz_intro.ipynb) notebook.\n",
    "\n",
    "First, we create a single curve for a single RCP scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and std with the previously defined function\n",
    "total_volume_mean, total_volume_std = calculate_total_mean_and_std(ds_merged, 'volume')\n",
    "\n",
    "# select only on RCP scenario\n",
    "total_volume_mean_rcp85 = total_volume_mean.loc[{'RCP': 'rcp85'}]\n",
    "\n",
    "# plot a curve\n",
    "x = total_volume_mean_rcp85.coords['time']\n",
    "y = total_volume_mean_rcp85\n",
    "hv.Curve((x, y),\n",
    "         kdims=x.name,\n",
    "         vdims=y.name,\n",
    "        ).opts(xlabel=x.attrs['description'],\n",
    "               ylabel=f\"{y.attrs['description']} in {y.attrs['unit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e3a3f",
   "metadata": {},
   "source": [
    "We used a [HoloViews Curve](https://holoviews.org/reference/elements/bokeh/Curve.html) and defined ```kdims``` and ```vdims```. This definition is not so important for a single plot but if we start to compose different plots all axis of the different plots with the same ```kdims``` or ```vdims``` are connected. This means for example whenever you zoom in on one plot all other plots also zoom in. Further, you can see that we have defined ```xlabel``` and ```ylabel``` using the variable description of the dataset, therefore it was useful to ```keep_attrs=True``` when we calculated the total values (see above).\n",
    "\n",
    "As a next step we can add the std as a shaded area ([HoloViews Area](https://holoviews.org/reference/elements/bokeh/Area.html)) and again define the whole plot as a single curve in a new function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd65edd",
   "metadata": {},
   "source": [
    "### Create single mean curve with std area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_curve(mean, std, rcp):\n",
    "    \n",
    "    mean_use = mean.loc[{'RCP': rcp}]  # read out the mean of the RCP to plot \n",
    "    std_use = std.loc[{'RCP': rcp}]  # read out the std of the RCP to plot\n",
    "    time = mean.coords['time']  # get the time for the x axis\n",
    "    \n",
    "    return (hv.Area((time,  # plot std as an area\n",
    "                     mean_use + std_use,  # upper boundary of the area\n",
    "                     mean_use - std_use),  # lower boundary of the area\n",
    "                    vdims=[mean_use.name, 'y2'],  # vdims for both boundaries\n",
    "                    kdims='time',\n",
    "                    label=rcp,\n",
    "                   ).opts(alpha=0.2,\n",
    "                          line_color=None,\n",
    "                         ) *\n",
    "            hv.Curve((time, mean_use),\n",
    "                     vdims=mean_use.name,\n",
    "                     kdims='time',\n",
    "                     label=rcp,\n",
    "                    )\n",
    "           ).opts(width=400,  # width of the total plot\n",
    "                  height=400,  # height of the total plot\n",
    "                  xlabel=time.attrs['description'],\n",
    "                  ylabel=f\"{mean_use.attrs['description']} in {mean_use.attrs['unit']}\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb58fb",
   "metadata": {},
   "source": [
    "### Overlay different scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba3353",
   "metadata": {},
   "source": [
    "The single curves of the different scenarios we can put together in a [HoloViews HoloMap](https://holoviews.org/reference/containers/bokeh/HoloMap.html), which is comparable to a dictionary. This further can be easily used to create a nice overlay of all curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_scenarios(ds, variable):\n",
    "    hmap = hv.HoloMap(kdims='Scenarios')   # create a HoloMap\n",
    "    mean, std = calculate_total_mean_and_std(ds, variable)  # calculate mean and std for all RCPs using our previously defined function\n",
    "    for rcp in all_rcp:\n",
    "        hmap[rcp] = get_single_curve(mean, std, rcp)  # add a curve for each RCP to the HoloMap, using the RCP as a key (when you compare it do a dictonary)\n",
    "    return hmap.overlay().opts(title=variable)  # create an overlay of all curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef819c",
   "metadata": {},
   "source": [
    "### Show different variables in one figure and save as html file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f4b26",
   "metadata": {},
   "source": [
    "Now that we have defined how our plots should look like we can compose different variables we want to explore in one plot. To do so we can use Panel [Column](https://panel.holoviz.org/reference/layouts/Column.html) and [Row](https://panel.holoviz.org/reference/layouts/Row.html) for customization of the plot layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plots = pn.Column(pn.Row(overlay_scenarios(ds_merged, 'volume'),\n",
    "                             overlay_scenarios(ds_merged, 'area'),\n",
    "                            ),\n",
    "                      overlay_scenarios(ds_merged, 'melt_on_glacier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06edfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3c2cc",
   "metadata": {},
   "source": [
    "When you start exploring the plots by dragging them around or zooming in (using the tools of the toolboxes in the upper right corner of each plot) you see that the x-axes are connected. This makes it very convenient for example to look at different periods for all variables interactively.\n",
    "\n",
    "You also can open the plots in a new browser tab by using\n",
    "\n",
    "```python\n",
    "all_plots.show()\n",
    "```\n",
    "\n",
    "or save it as a html file for sharing\n",
    "\n",
    "```python\n",
    "plots_to_save = pn.panel(all_plots)\n",
    "plots_to_save.save('GCM_runs.html', embed=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248814d",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "- return to the [OGGM documentation](https://docs.oggm.org)\n",
    "- back to the [table of contents](welcome.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
