{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toy statistical model of ice thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook primarily introduces the `gis.gridded_attributes` and `gis.gridded_mb_attributes` tasks which are simply adding certain diagnostic variables to the glacier maps such as slope, aspect, distance from border, and other mass-balance related variables which might be useful to some people.\n",
    "\n",
    "We also show how to use these data to build a very simple (and not very good) regression model of ice thickness. This second part (\"Build a statistical model\") is rather technical and not directly related to OGGM, so you might stop at this point unless your are into the topic yourself. You will need to install [scikit-learn](https://scikit-learn.org/stable/index.html) for the second part of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the South Glacier example taken from the [ITMIX experiment](https://www.the-cryosphere.net/11/949/2017/). We'll also use the data provided with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import salem\n",
    "\n",
    "# OGGM\n",
    "import oggm.cfg as cfg\n",
    "from oggm import utils, workflow, tasks, graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OGGM and set up the default run parameters\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "cfg.PARAMS['use_multiprocessing'] = False\n",
    "# Local working directory (where OGGM will write its output)\n",
    "cfg.PATHS['working_dir'] = utils.gettempdir('OGGM_Toy_Thickness_Model')\n",
    "# We start at level 3, because we need all data for the attributes\n",
    "gdirs = workflow.init_glacier_directories(['RGI60-01.16195'], from_prepro_level=3, prepro_border=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the tasks adding the attributes to the gridded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested tasks\n",
    "task_list = [\n",
    "    tasks.gridded_attributes,\n",
    "    tasks.gridded_mb_attributes,\n",
    "]\n",
    "for task in task_list:\n",
    "    workflow.execute_entity_task(task, gdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick our glacier\n",
    "gdir = gdirs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gridded attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the gridded data file with xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(gdir.get_filepath('gridded_data'))\n",
    "# List all variables\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains several variables with their description. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.oggm_mb_above_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of them (we show how to plot them with xarray and with oggm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.slope.plot();\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "graphics.plot_raster(gdir, var_name='aspect', cmap='twilight', ax=ax1)\n",
    "graphics.plot_raster(gdir, var_name='oggm_mb_above_z', ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve these attributes at point locations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this glacier, we have ice thickness observations (all downloaded from the supplementary material of the [ITMIX paper](https://www.the-cryosphere.net/11/949/2017/)). Let's make a table out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = salem.read_shapefile(utils.get_demo_file('IceThick_SouthGlacier.shp'))\n",
    "coords = np.array([p.xy for p in df.geometry]).squeeze()\n",
    "df['lon'] = coords[:, 0]\n",
    "df['lat'] = coords[:, 1]\n",
    "df = df[['lon', 'lat', 'thick']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the longitudes and latitudes to the glacier map projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = salem.transform_proj(salem.wgs84, gdir.grid.proj, df['lon'].values, df['lat'].values)\n",
    "df['x'] = xx\n",
    "df['y'] = yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = gdir.read_shapefile('outlines')\n",
    "f, ax = plt.subplots()\n",
    "df.plot.scatter(x='x', y='y', c='thick', cmap='viridis', s=10, ax=ax);\n",
    "geom.plot(ax=ax, facecolor='none', edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: interpolation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurement points of this dataset are very frequent and close to each other. There are plenty of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will keep them all and interpolate the variables of interest at the point's location. We use [xarray](http://xarray.pydata.org/en/stable/interpolation.html#advanced-interpolation) for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vns = ['topo',\n",
    "       'slope',\n",
    "       'slope_factor',\n",
    "       'aspect',\n",
    "       'dis_from_border',\n",
    "       'catchment_area',\n",
    "       'lin_mb_above_z',\n",
    "       'lin_mb_above_z_on_catch',\n",
    "       'oggm_mb_above_z',\n",
    "       'oggm_mb_above_z_on_catch',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate (bilinear)\n",
    "for vn in vns:\n",
    "    df[vn] = ds[vn].interp(x=('z', df.x), y=('z', df.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these variables can explain the measured ice thickness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "df.plot.scatter(x='dis_from_border', y='thick', ax=ax1);\n",
    "df.plot.scatter(x='slope', y='thick', ax=ax2);\n",
    "df.plot.scatter(x='oggm_mb_above_z', y='thick', ax=ax3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a negative correlation with slope (as expected), a positive correlation with the mass-flux (oggm_mb_above_z), and a slight positive correlation with the distance from the glacier boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: aggregated per grid point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so many points that much of the information obtained by OGGM is interpolated. A way to deal with this is to aggregate all the measurement points per grid point and to average them. Let's do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df[['lon', 'lat', 'thick']].copy()\n",
    "ii, jj = gdir.grid.transform(df['lon'], df['lat'], crs=salem.wgs84, nearest=True)\n",
    "df_agg['i'] = ii\n",
    "df_agg['j'] = jj\n",
    "# We trick by creating an index of similar i's and j's\n",
    "df_agg['ij'] = ['{:04d}_{:04d}'.format(i, j) for i, j in zip(ii, jj)]\n",
    "df_agg = df_agg.groupby('ij').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select\n",
    "for vn in vns:\n",
    "    df_agg[vn] = ds[vn].isel(x=('z', df_agg.i), y=('z', df_agg.j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 9 times less points, but the main features of the data remain unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "df_agg.plot.scatter(x='dis_from_border', y='thick', ax=ax1);\n",
    "df_agg.plot.scatter(x='slope', y='thick', ax=ax2);\n",
    "df_agg.plot.scatter(x='oggm_mb_above_z', y='thick', ax=ax3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a statistical model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use scikit-learn to build a very simple linear model of ice-thickness. First, we have to acknowledge that there is a correlation between many of the explanatory variables we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8));\n",
    "sns.heatmap(df.corr(), cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a problem for linear regression models, which cannot deal well with correlated explanatory variables. We have to do a so-called \"feature selection\", i.e. keep only the variables which are independently important to explain the outcome.\n",
    "\n",
    "For the sake of simplicity, let's use the [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) method to help us out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # sklearn sends a lot of warnings\n",
    "# Scikit learn (can be installed e.g. via conda install -c anaconda scikit-learn)\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our data\n",
    "df = df.dropna()\n",
    "# Variable to model\n",
    "target = df['thick']\n",
    "# Predictors - remove x and y (redundant with lon lat)\n",
    "# Normalize it in order to be able to compare the factors\n",
    "data = df.drop(['thick', 'x', 'y'], axis=1).copy()\n",
    "data_mean = data.mean()\n",
    "data_std = data.std()\n",
    "data = (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to select the regularization parameter\n",
    "lasso_cv = LassoCV(cv=5, random_state=0)\n",
    "lasso_cv.fit(data.values, target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a statistical model trained on the full dataset. Let's see how it compares to the calibration data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = df.copy()\n",
    "odf['thick_predicted'] = lasso_cv.predict(data.values)\n",
    "f, ax = plt.subplots(figsize=(6, 6));\n",
    "odf.plot.scatter(x='thick', y='thick_predicted', ax=ax);\n",
    "plt.xlim([-25, 220]);\n",
    "plt.ylim([-25, 220]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so well. It is doing OK for low thicknesses, but high thickness are strongly underestimated. Which explanatory variables did the model choose as being the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.Series(lasso_cv.coef_, index=data.columns)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. Lons and lats have a predictive power over thickness? Unfortunately, this is more a coincidence than a reality. If we look at the correlation of the error with the variables of importance, one sees that there is a large correlation between the error and the spatial variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf['error'] = np.abs(odf['thick_predicted'] - odf['thick'])\n",
    "odf.corr()['error'].loc[predictors.loc[predictors != 0].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the model is not very robust. Let's use cross-validation to check our model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(4, random_state=0, shuffle=True)\n",
    "for k, (train, test) in enumerate(k_fold.split(data.values, target.values)):\n",
    "    lasso_cv.fit(data.values[train], target.values[train])\n",
    "    print(\"[fold {0}] alpha: {1:.5f}, score (r^2): {2:.5f}\".\n",
    "          format(k, lasso_cv.alpha_, lasso_cv.score(data.values[test], target.values[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the hyper-parameter alpha and the score change that much between iterations is a sign that the model isn't very robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model is bad, but... let's apply it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the model to our entre glacier, we have to get the explanatory variables from the gridded dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variables we are missing\n",
    "lon, lat = gdir.grid.ll_coordinates\n",
    "ds['lon'] = (('y', 'x'), lon)\n",
    "ds['lat'] = (('y', 'x'), lat)\n",
    "\n",
    "# Generate our dataset\n",
    "pred_data = pd.DataFrame()\n",
    "for vn in data.columns:\n",
    "    pred_data[vn] = ds[vn].data[ds.glacier_mask == 1]\n",
    "\n",
    "# Normalize using the same normalization constants\n",
    "pred_data = (pred_data - data_mean) / data_std\n",
    "\n",
    "# Apply the model\n",
    "pred_data['thick'] = lasso_cv.predict(pred_data.values)\n",
    "\n",
    "# For the sake of physics, clip negative thickness values...\n",
    "pred_data['thick'] = np.clip(pred_data['thick'], 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to 2d and in xarray\n",
    "var = ds[vn].data * np.NaN\n",
    "var[ds.glacier_mask == 1] = pred_data['thick']\n",
    "ds['linear_model_thick'] = (('y', 'x'), var)\n",
    "ds['linear_model_thick'].attrs['description'] = 'Predicted thickness'\n",
    "ds['linear_model_thick'].attrs['units'] = 'm'\n",
    "ds['linear_model_thick'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have shown how to compute gridded attributes from OGGM glaciers such as slope, aspect, or catchments\n",
    "- we used two methods to extract these data at point locations: with interpolation or with aggregated averages on each grid point\n",
    "- as an application example, we trained a linear regression model to predict the ice thickness of this glacier at unseen locations\n",
    "\n",
    "The model we developed was quite bad and we used quite lousy statistics. If I had more time to make it better, I would:\n",
    "- make a pre-selection of meaningful predictors to avoid discontinuities\n",
    "- use a non-linear model\n",
    "- use cross-validation to better asses the true skill of the model\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "- return to the [OGGM documentation](https://docs.oggm.org)\n",
    "- back to the [table of contents](welcome.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: how does the statistical model compare to OGGM \"out-of the box\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the dataset cause we are going to write in it\n",
    "ds = ds.load()\n",
    "ds.close()\n",
    "ds.to_netcdf(gdir.get_filepath('gridded_data'))\n",
    "# Distribute thickness using default values only\n",
    "workflow.execute_entity_task(tasks.distribute_thickness_per_altitude, gdirs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the linear model data for comparison\n",
    "ds = xr.open_dataset(gdir.get_filepath('gridded_data'))\n",
    "df_agg['oggm_thick'] = ds.distributed_thickness.isel(x=('z', df_agg['i']), y=('z', df_agg['j']))\n",
    "df_agg['linear_model_thick'] = ds.linear_model_thick.isel(x=('z', df_agg['i']), y=('z', df_agg['j']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5));\n",
    "ds['linear_model_thick'].plot(ax=ax1);\n",
    "ds['distributed_thickness'].plot(ax=ax2);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6));\n",
    "df_agg.plot.scatter(x='thick', y='linear_model_thick', ax=ax1);\n",
    "ax1.set_xlim([-25, 220]);\n",
    "ax1.set_ylim([-25, 220]);\n",
    "df_agg.plot.scatter(x='thick', y='oggm_thick', ax=ax2);\n",
    "ax2.set_xlim([-25, 220]);\n",
    "ax2.set_ylim([-25, 220]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
