{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4e5293",
   "metadata": {},
   "source": [
    "# Merge, analyse and visualize OGGM GCM runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79fd02",
   "metadata": {},
   "source": [
    "In this notebook we want to show: \n",
    "- How to merge the output of different OGGM GCM projections into one dataset\n",
    "- How to calculate regional values (e.g. volume) using the merged dataset\n",
    "- How to use matplotlib or [seaborn](https://seaborn.pydata.org/index.html) to visualize the projections and plot different statistical estimates (median, interquartile range, mean, std, ...)\n",
    "- How to use [HoloViews](https://holoviews.org/) and [Panel](https://panel.holoviz.org/) to visualize the outcome. This part uses advanced plotting capabilities which are not necessary to understand the rest of the notebook.\n",
    "\n",
    "This notebook is intended to explain the postprocessing steps, rather than the OGGM workflow itself. Therefore some code (especially conducting the GCM projection runs) does not have many explanations. If you are more interested in these steps you should check out the notebook [Run OGGM with GCM data](run_with_gcm.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d387c",
   "metadata": {},
   "source": [
    "## GCM projection runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fdf37",
   "metadata": {},
   "source": [
    "The first step is to conduct the GCM projection runs. We choose two different glaciers by their rgi_ids and conduct the GCM projections. Again if you do not understand all of the following code you should check out the [Run OGGM with GCM data](run_with_gcm.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fc546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libs\n",
    "from time import gmtime, strftime\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Locals\n",
    "from oggm import utils, workflow, tasks\n",
    "import oggm.cfg as cfg\n",
    "from oggm.shop import gcm_climate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c0a56",
   "metadata": {},
   "source": [
    "### Pre-processed directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493ee10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize OGGM and set up the default run parameters\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "\n",
    "# change border around the individual glaciers\n",
    "cfg.PARAMS['border'] = 80\n",
    "\n",
    "# Use Multiprocessing\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "\n",
    "# For hydro output\n",
    "cfg.PARAMS['store_model_geometry'] = True\n",
    "\n",
    "# Local working directory (where OGGM will write its output)\n",
    "cfg.PATHS['working_dir'] = utils.gettempdir('OGGM_merge_gcm_runs', reset=True)\n",
    "\n",
    "# RGI glaciers: Ngojumba and Khumbu\n",
    "rgi_ids = utils.get_rgi_glacier_entities(['RGI60-15.03473', 'RGI60-15.03733'])\n",
    "\n",
    "# Go - get the pre-processed glacier directories\n",
    "# in OGGM v1.6 you have to explicitly indicate the url from where you want to start from\n",
    "# we will use here the elevation band flowlines which are much simpler than the centerlines\n",
    "base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5/')\n",
    "gdirs = workflow.init_glacier_directories(rgi_ids, from_prepro_level=5,\n",
    "                                          prepro_base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3475a8",
   "metadata": {},
   "source": [
    "### Download and process GCM data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124be71-9d5d-4b03-a22b-f51a87d0acfb",
   "metadata": {},
   "source": [
    "We prepared for you two options:\n",
    "- You can either use the ISIMIP3b files that are bias-corrected CMIP6 GCMs\n",
    "- or use directly CMIP5 (differences explained shortly in [run_with_gcm](run_with_gcm.ipynb)).\n",
    "We will use here per default ISIMIP3b, because then you need to download less data. \n",
    "\n",
    "The subsequent postprocessing and plotting code works for both options. If you want to do the visualization with Holoviews, it only works with `option='CMIP5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42420e7f-878c-4f38-a135-a4a1ee76b964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "option = 'isimip3b_CMIP6'\n",
    "# option = 'CMIP5'    ## Attention: This takes quite long and needs to download much more data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48946199-5ecf-4054-810b-0da6070a5bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if option == 'isimip3b_CMIP6':\n",
    "    all_GCM = ['gfdl-esm4_r1i1p1f1', 'ipsl-cm6a-lr_r1i1p1f1',\n",
    "               'mpi-esm1-2-hr_r1i1p1f1',\n",
    "               'mri-esm2-0_r1i1p1f1', 'ukesm1-0-ll_r1i1p1f2']\n",
    "\n",
    "    # define the SSP scenarios\n",
    "    all_scenario = ['ssp126', 'ssp370', 'ssp585']\n",
    "\n",
    "    for GCM in all_GCM:\n",
    "        for ssp in all_scenario:\n",
    "            # Download and process them:\n",
    "            workflow.execute_entity_task(gcm_climate.process_monthly_isimip_data, gdirs, \n",
    "                                         ssp = ssp,\n",
    "                                         # gcm ensemble -> you can choose another one\n",
    "                                         member=GCM,\n",
    "                                         # recognize the climate file for later\n",
    "                                         output_filesuffix=f'_{GCM}_{ssp}'\n",
    "                                         );\n",
    "elif option=='CMIP5':\n",
    "    # define the GCM models you want to use\n",
    "    all_GCM = ['CCSM4',\n",
    "               'CNRM-CM5',\n",
    "               'CSIRO-Mk3-6-0',\n",
    "              ]\n",
    "\n",
    "    # define the SSP scenarios to use\n",
    "    all_scenario = ['rcp26',\n",
    "                   #'rcp45',# let's only take three scenarios, that is faster\n",
    "                   'rcp60', \n",
    "                   'rcp85']\n",
    "\n",
    "    # download locations for precipitation and temperature\n",
    "    bp = 'https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/pr/pr_mon_{}_{}_r1i1p1_g025.nc'\n",
    "    bt = 'https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/tas/tas_mon_{}_{}_r1i1p1_g025.nc'\n",
    "\n",
    "    for GCM in all_GCM:\n",
    "        for rcp in all_scenario:\n",
    "            # Download the files\n",
    "            ft = utils.file_downloader(bt.format(GCM, rcp))\n",
    "            fp = utils.file_downloader(bp.format(GCM, rcp))\n",
    "\n",
    "            try:\n",
    "                # bias correct them\n",
    "                workflow.execute_entity_task(gcm_climate.process_cmip_data, gdirs, \n",
    "                                             filesuffix='_{}_{}'.format(GCM, rcp),  # recognize the climate file for later\n",
    "                                             fpath_temp=ft,  # temperature projections\n",
    "                                             fpath_precip=fp,  # precip projections\n",
    "                                             );\n",
    "            except ValueError:\n",
    "                # if a certain scenario is not available for a GCM we land here\n",
    "                # and we inidcate this by printing a message so the user knows\n",
    "                # this scenario is missing\n",
    "                print('No ' + GCM +' run with scenario ' + rcp + ' available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56314005",
   "metadata": {},
   "source": [
    "Here we defined, downloaded and processed all the GCM models and scenarios (either SSP or RCP) that we want to use. In case of CMIP5, you can get [here](https://cluster.klima.uni-bremen.de/~oggm/cmip5-ng/gcm_table.html) an overview of all available options. As you see in the overview only for two of the three GCMs for CMIP5 the rcp60 Scenario is available! We deal with this issue by including a ```try```/```except``` in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c8587",
   "metadata": {},
   "source": [
    "### Actual projection Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b079e",
   "metadata": {},
   "source": [
    "Now we conduct the actual projection runs. Again handling the case that for a certain (GCM, SCENARIO) combination no data is available with a ```try```/```except```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29abdbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for GCM in all_GCM:\n",
    "    for scen in all_scenario:\n",
    "        rid = '_{}_{}'.format(GCM, scen)\n",
    "        try:  # check if (GCM, scen) combination exists\n",
    "            workflow.execute_entity_task(tasks.run_with_hydro, gdirs,\n",
    "                                         run_task=tasks.run_from_climate_data,\n",
    "                                         ys=2020,  # star year of our projection runs\n",
    "                                         climate_filename='gcm_data',  # use gcm_data, not climate_historical\n",
    "                                         climate_input_filesuffix=rid,  # use the chosen GCM and scenario\n",
    "                                         init_model_filesuffix='_historical',  # this is important! Start from 2020 glacier\n",
    "                                         output_filesuffix=rid,  # the filesuffix of the resulting file, so we can find it later\n",
    "                                         store_monthly_hydro=True\n",
    "                                        );\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            # if a certain scenario is not available for a GCM we land here\n",
    "            # and we inidcate this by printing a message so the user knows\n",
    "            # this scenario is missing\n",
    "            print('No ' + GCM +' run with scenario ' + scen + ' available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12feb23a",
   "metadata": {},
   "source": [
    "## Merge datasets together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f791e8c",
   "metadata": {},
   "source": [
    "Now that we have conducted the projection Runs we can start merging all outputs into one dataset. The individual files can be opened by providing the ```output_filesuffix``` of the projection runs as the ```input_filesuffix```. Let us have a look at one resulting dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc144a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_id  = '_{}_{}'.format(all_GCM[0], all_scenario[0])\n",
    "ds = utils.compile_run_output(gdirs, input_filesuffix=file_id)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a29a8",
   "metadata": {},
   "source": [
    "We see that as a result, we get a [xarray.Dataset](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.html). We also see that this dataset has three dimensions ```time```, ```rgi_id``` and ```month_2d```. When we look at the ```Data variables``` (click to expand) we can see that for each of these variables a certain combination of the dimensions is given. For example, for the ```volume``` we see the dimensions ```(time, rgi_id)``` which indicates that the underlying data is separated by each year and each glacier. Let's have a look at the volume in 2030 and 2040 at the glacier 'RGI60-15.03473' (Ngojumba):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521cf66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['volume'].loc[{'time': [2030, 2040], 'rgi_id': ['RGI60-15.03473' ]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d18aa6",
   "metadata": {},
   "source": [
    "A more complete overview of how to access the data of xarray Dataset can be found [here](https://xarray.pydata.org/en/stable/user-guide/indexing.html#dataset-indexing).\n",
    "\n",
    "This is quite useful, but unfortunately, we have one xarray Dataset for each (GCM, RCP) pair. For an easier calculation and comparison between different GCMs and Scenarios(SSPs or RCPs), it would be desirable to combine all individual Datasets into one Dataset with the additional dimensions ```(GCM, SCENARIO)```. Fortunately, xarray provides different functions to merge Datasets ([here](https://xarray.pydata.org/en/stable/user-guide/combining.html) you can find an Overview).\n",
    "\n",
    "In our case, we want to merge all individual datasets with two additional coordinates ```GCM``` and ```SCENARIO```. Additional coordinates can be added with the comment ```.coords[]```, also you can include a description with ```.coords[].attrs['description']```. For example, let us add ```GCM``` as a new coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5dbb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.coords['GCM'] = all_GCM[0]\n",
    "ds.coords['GCM'].attrs['description'] = 'Global Circulation Model'\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67effd31",
   "metadata": {},
   "source": [
    "You can see that now ```GCM``` is added as Coordinate, but when you look at the Data variables you see that this new Coordinate is not used by any of them. So we must add this new Coordinate to the Data variables using the ```.expand_dims()``` comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33361f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds.expand_dims('GCM')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052351e",
   "metadata": {},
   "source": [
    "Now we see that ```GCM``` was added to the Dimensions and all Data variables now use this new coordinate. The same can be done with ```SCENARIO```. As a standalone, this is not very useful. But if we add these coordinates to all datasets, it becomes quite handy for merging. Therefore we now open all datasets and add to each one the two coordinates ```GCM``` and ```SCENARIO```:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e7302",
   "metadata": {},
   "source": [
    "### Add new coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004874c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_all = []  # in this array all datasets going to be stored with additional coordinates GCM and RCP\n",
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "for GCM in all_GCM:  # loop through all GCMs\n",
    "    for scen in all_scenario:  # loop through all RCPs\n",
    "        try:  # check if GCM, RCP combination exists\n",
    "            rid = '_{}_{}'.format(GCM, scen)  # put together the same filesuffix which was used during the projection runs\n",
    "\n",
    "            ds_tmp = utils.compile_run_output(gdirs, input_filesuffix=rid)  # open one model run\n",
    "\n",
    "            ds_tmp.coords['GCM'] = GCM  # add GCM as a coordinate\n",
    "            ds_tmp.coords['GCM'].attrs['description'] = 'used Global circulation Model'  # add a description for GCM\n",
    "            ds_tmp = ds_tmp.expand_dims(\"GCM\")  # add GCM as a dimension to all Data variables\n",
    "\n",
    "            ds_tmp.coords['SCENARIO'] = scen  # add scenario (ssp or rcp) as a coordinate\n",
    "            ds_tmp.coords['SCENARIO'].attrs['description'] = 'used scenario (SSP or RCP)'\n",
    "            ds_tmp = ds_tmp.expand_dims(\"SCENARIO\")  # add RCP as a dimension to all Data variables\n",
    "\n",
    "            ds_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "            ds_all.append(ds_tmp)  # add the dataset with extra coordinates to our final ds_all array\n",
    "\n",
    "        except RuntimeError as err:  # here we land if an error occured\n",
    "            if str(err) == 'Found no valid glaciers!':  # This is the error message if the GCM, RCP combination does not exist\n",
    "                print(f'No data for GCM {GCM} with SCENARIO {scen} found!')  # print a descriptive message\n",
    "            else:\n",
    "                raise RuntimeError(err)  # if an other error occured we just raise it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d149089",
   "metadata": {},
   "source": [
    "### Actual merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4a1b4",
   "metadata": {},
   "source": [
    "```ds_all``` now contains all outputs with the additionally added coordinates. Now we can merge them with the very convenient xarray function ```xr.combine_by_coords()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba2b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_merged = xr.combine_by_coords(ds_all, fill_value=np.nan)  # define how the missing GCM, RCP combinations should be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5a4f6",
   "metadata": {},
   "source": [
    "Great! You see that the result is one Dataset, but we still can identify the individual runs with the two additionally added Coordinates GCM and RCP. If you want to save this new Dataset for later analysis you can use for example ```.to_netcdf()``` ([here](https://xarray.pydata.org/en/stable/user-guide/io.html) you can find an Overview of xarray writing options):\n",
    "\n",
    "```python\n",
    "ds_merged.to_netcdf('merged_GCM_projections.nc')\n",
    "```\n",
    "\n",
    "And to open it again later and automatically close the file after reading you can use:\n",
    "\n",
    "```python\n",
    "with xr.open_dataset('merged_GCM_projections.nc') as ds:\n",
    "    ds_merged = ds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a0539",
   "metadata": {},
   "source": [
    "## Calculate regional values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd816e6",
   "metadata": {},
   "source": [
    "Now we have our merged dataset and we can start with the analysis. To do this xarray gives us a ton of different mathematical functions which can be used directly with our dataset. [Here](https://xarray.pydata.org/en/stable/user-guide/computation.html) you can find a great overview of what is possible.\n",
    "\n",
    "As an example, we could calculate the mean and std of the total regional glacier volume for different scenarios of our merged dataset. Here our added coordinates come in very handy. However, for a small amount of GCMs (for ISIMIP3b e.g. 5 GCMs), it is often better to compute instead more robust estimates, such as the median, the 25th and 75th percentile or the total range. We will show different options later:\n",
    "\n",
    "Let us start by calculating the total \"regional\" glacier volume using ```.sum()``` (of course, here is is just the sum over two glaciers...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1b16e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_total_volume = ds_merged['volume'].sum(dim='rgi_id',  # over which dimension the sum should be taken, here we want to sum up over all glacier ids\n",
    "                                          skipna=True,  # ignore nan values\n",
    "                                          # important, we need values for every glacier (here 2) to do the sum\n",
    "                                          # if some have NaN, the sum should also be NaN (if you don't set min_count, the sum will be 0, which is bad)\n",
    "                                          min_count=len(ds_merged.rgi_id), \n",
    "                                          keep_attrs=True)  # keep the variable descriptions                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7cfcf",
   "metadata": {},
   "source": [
    "For this calculation, we told xarray over which dimension we want to sum up ```dim='rgi_id'``` (all glaciers ), that missing values should be ignored ```skipna=True``` (as some GCM/RCP combinations are not available) and that the attributes (the description of the variables) should be preserved ```keep_attrs=True```. You can see that the result has three dimensions left ```(GCM, time, RCP)``` and the dimension ```rgi_id``` disappeared as we have summed all values up along this dimension.\n",
    "\n",
    "Likewise, we can now calculate for example the median of all GCM runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf62bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_total_volume_median = ds_total_volume.median(dim='GCM',  # over which dimension the median should be calculated\n",
    "                                            skipna=True,  # ignore nan values\n",
    "                                            keep_attrs=True)  # keep all variable descriptions\n",
    "ds_total_volume_median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237d48e",
   "metadata": {},
   "source": [
    "And you see that now we are left with two dimensions ```(SCENARIO, time)```. This means we have calculated the median total volume for all different scenarios (SSP or RCP) and along the projection period. The mean, standard deviation (std) or percentiles can be also calculated in the same way as the median. Again, bare in mind, that for small sample sizes of GCM ensembles (15), which is almost always the case, it is often much more robust to use median and the interquartile range or other percentile ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a10d69",
   "metadata": {},
   "source": [
    "## Visualisation with matplotlib or seaborn\n",
    "We will first use matplotlib, where we need to estimate the statistical estimates ourselves. This is good to understand what we are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502aab2a-97c0-4950-ac69-6f777aaa6c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80301c95-a031-4715-9910-3e4fcfcd3300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimate the minimum volume for each time step and scenario over the GCMs\n",
    "ds_total_volume_min = ds_total_volume.min(dim='GCM', keep_attrs=True,skipna=True)\n",
    "# estimate the 25th percentile volume for each time step and scenario over the GCMs\n",
    "ds_total_volume_p25 = ds_total_volume.quantile(0.25, dim='GCM', keep_attrs=True,skipna=True)\n",
    "# estimate the 50th percentile volume for each time step and scenario over the GCMs\n",
    "ds_total_volume_p50 = ds_total_volume.quantile(0.5, dim='GCM', keep_attrs=True,skipna=True)\n",
    "# this is the same as the median -> Let's check\n",
    "np.testing.assert_allclose(ds_total_volume_median,ds_total_volume_p50)\n",
    "# estimate the 75th percentile volume for each time step and scenario over the GCMs\n",
    "ds_total_volume_p75 = ds_total_volume.quantile(0.75, dim='GCM', keep_attrs=True,skipna=True)\n",
    "# estimate the maximum volume for each time step and scenario over the GCMs\n",
    "ds_total_volume_max = ds_total_volume.max(dim='GCM', keep_attrs=True,skipna=True)\n",
    "\n",
    "# Think twice if it is appropriate to compute a mean/std over your GCM sample, is it Gaussian distributed?\n",
    "# Otherwise use instead median and percentiles or total range\n",
    "ds_total_volume_mean = ds_total_volume.mean(dim='GCM', keep_attrs=True,\n",
    "                                            skipna=True)\n",
    "ds_total_volume_std = ds_total_volume.std(dim='GCM', keep_attrs=True,\n",
    "                                            skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a970a-03a8-45a3-9942-58309dd2aed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if option == 'isimip3b_CMIP6':\n",
    "    color_dict={'ssp126':'blue', 'ssp370':'orange', 'ssp585':'red'}\n",
    "elif option == 'CMIP5':\n",
    "    color_dict={'rcp26':'blue', 'rcp60':'violet', 'rcp85':'red'}\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6), \n",
    "                        sharey=True # we want to share the y axis betweeen the subplots\n",
    "                       )\n",
    "for scenario in color_dict.keys():\n",
    "    # get amount of GCMs per Scenario to add it to the legend:\n",
    "    n = len(ds_total_volume.sel(SCENARIO=scenario).dropna(dim='GCM').GCM)\n",
    "    axs[0].plot(ds_total_volume_median.time,\n",
    "                ds_total_volume_median.sel(SCENARIO=scenario)/1e9,  # m3 -> km3\n",
    "                label=f'{scenario}: n={n} GCMs',\n",
    "                color=color_dict[scenario],lw=3)\n",
    "    axs[0].fill_between(ds_total_volume_median.time,\n",
    "                        ds_total_volume_p25.sel(SCENARIO=scenario)/1e9,\n",
    "                        ds_total_volume_p75.sel(SCENARIO=scenario)/1e9,\n",
    "                        color=color_dict[scenario],\n",
    "                        alpha=0.5,\n",
    "                        label='interquartile range\\n(75th-25th percentile)')\n",
    "    axs[0].fill_between(ds_total_volume_median.time,\n",
    "                        ds_total_volume_min.sel(SCENARIO=scenario)/1e9,\n",
    "                        ds_total_volume_max.sel(SCENARIO=scenario)/1e9,\n",
    "                        color=color_dict[scenario],\n",
    "                        alpha=0.1,\n",
    "                        label='total range')\n",
    "\n",
    "\n",
    "for scenario in color_dict.keys():\n",
    "    axs[1].plot(ds_total_volume_mean.time,\n",
    "                 ds_total_volume_mean.sel(SCENARIO=scenario)/1e9,  # m3 -> km3\n",
    "                 color=color_dict[scenario],\n",
    "                 label='mean',lw=3)\n",
    "    axs[1].fill_between(ds_total_volume_mean.time,\n",
    "                        ds_total_volume_mean.sel(SCENARIO=scenario)/1e9 - ds_total_volume_std.sel(SCENARIO=scenario)/1e9,\n",
    "                        ds_total_volume_mean.sel(SCENARIO=scenario)/1e9 + ds_total_volume_std.sel(SCENARIO=scenario)/1e9,\n",
    "                        alpha=0.3,\n",
    "                        color=color_dict[scenario],\n",
    "                       label='standard deviation')\n",
    "    \n",
    "for ax in axs:\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if ax == axs[0]:\n",
    "        leg1 = ax.legend(handles[:3], labels[:3], title='Scenario')\n",
    "        ax.set_ylabel(r'Volume (km$^3$)')\n",
    "        ax.legend([handles[0],handles[3],handles[4]],\n",
    "                  ['median',labels[3],labels[4]], loc='lower left')\n",
    "        ax.add_artist(leg1)\n",
    "    else:\n",
    "        # we only want to have the legend for mean and std\n",
    "        ax.legend(handles[::3], labels[::3], loc='lower left') \n",
    "    ax.set_xlabel('Year');\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fd340-0538-4927-8cee-9a170a58d0e1",
   "metadata": {},
   "source": [
    "You have to choose which statistical estimate is best in your case. It is also a good possibility to just plot all the scenarios, and then to decide which statistical estimate describes best the spread in your case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba29e68-34f9-4395-accf-f8af8e468d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for example, if there are just 3 GCMs, maybe you can just show all of them?\n",
    "for gcm in all_GCM: \n",
    "    plt.plot(ds_total_volume.sel(SCENARIO=scenario).time,\n",
    "            ds_total_volume.sel(SCENARIO=scenario, GCM=gcm),\n",
    "             color='grey')\n",
    "plt.title(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ea4c3-7fdb-45c9-9b09-c7516bebe8a8",
   "metadata": {},
   "source": [
    "You could create similar plots even easier with  [seaborn](https://seaborn.pydata.org/index.html), which has a lot of statistical estimation tools directly included (specifically in seaborn>=v0.12). You can for example check out [this errorbar seaborn tutorial](https://seaborn.pydata.org/tutorial/error_bars.html). Note that the outcome can be slightly different even for the same statistical estimate as e.g. seaborn and xarray might use different methods to compute the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec7d23-b91e-430c-931e-c6136842e424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "# this code might only work with seaborn v>=0.12\n",
    "# seaborn always likes to have pandas dataframes\n",
    "pd_total_volume = ds_total_volume.to_dataframe('volume_m3').reset_index()\n",
    "sns.lineplot(x='time', y='volume_m3',\n",
    "             hue='SCENARIO', # these are the dimension for the different colors \n",
    "             estimator='median', # here you could also choose the mean\n",
    "             data=pd_total_volume,\n",
    "             # here you could also choose another percentile range\n",
    "             # \"90\" means the range goes from the 5th to the 95th percentile\n",
    "             # (50 would be the interquartile range, i.e., the same as two plots above)\n",
    "             errorbar=('pi', 90) \n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a116ec-affd-4071-8d76-be5123cdbc59",
   "metadata": {},
   "source": [
    "## Interactive visualisation with HoloViews and Panel\n",
    "- the following code only works if you install holoviews and panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bdb61-62d0-476c-9b03-d4cc4681dd8b",
   "metadata": {},
   "source": [
    "We can also visualize the data using tools from the [HoloViz](https://holoviz.org/) framework (namely [HoloViews](https://holoviews.org/) and [Panel](https://panel.holoviz.org/)). For an introduction to HoloViz, you can have a look at the [Small overview of HoloViz capability of data exploration](holoviz_intro.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f898fd-9042-45e4-88ed-be4e3f9104e4",
   "metadata": {},
   "source": [
    "This part of the code only works at the moment if you set \n",
    "option=`CMIP5` when you preprocess the projection data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6605825-8359-48f2-b578-bc65e26a14e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "hv.extension('bokeh')\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659cff7e-c1c3-43d8-9c7c-9ea67ee46665",
   "metadata": {},
   "source": [
    "To make your life easier with Holoviews and Panel, you can define a small function that computes your estimates:\n",
    "- In the following section, we only show mean and std, but this could be similar applied to the median and specific percentiles (which might be more robust in your use case :-) ). But just for showing how it works, it is fine to use here mean and std!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e87ea6-b54c-4584-80e2-8ce5a4f63529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_total_mean_and_std(ds, variable):\n",
    "    mean = ds[variable].sum(dim='rgi_id',  # first sum up over all glaciers\n",
    "                            skipna=True,\n",
    "                            keep_attrs=True,\n",
    "                            # important, need values for every glacier\n",
    "                            min_count=len(ds_merged.rgi_id), \n",
    "                           ).mean(dim='GCM',  # afterwards calculate the mean of all GCMs\n",
    "                                  skipna=True,\n",
    "                                  keep_attrs=True,\n",
    "                                 )\n",
    "    std = ds[variable].sum(dim='rgi_id',  # first sum up over all glaciers \n",
    "                           skipna=True,\n",
    "                           keep_attrs=True,\n",
    "                           min_count=len(ds_merged.rgi_id), \n",
    "                          ).std(dim='GCM',  # afterwards calculate the std of all GCMs\n",
    "                                skipna=True,\n",
    "                                keep_attrs=True,\n",
    "                               )\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4d9a5-ee56-4543-91bd-4d776523b090",
   "metadata": {},
   "source": [
    "This function takes a dataset ```ds``` and a variable name ```variable``` for which the mean and the std should be calculated. This function will come in handy in the next section dealing with the visualisation of the calculated values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f8d1c",
   "metadata": {},
   "source": [
    "First, we create a single curve for a single RCP scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b1aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate mean and std with the previously defined function\n",
    "total_volume_mean, total_volume_std = calculate_total_mean_and_std(ds_merged, 'volume')\n",
    "\n",
    "# select only on RCP scenario\n",
    "total_volume_mean_ssp585 = total_volume_mean.loc[{'SCENARIO': 'ssp585'}]\n",
    "\n",
    "# plot a curve\n",
    "x = total_volume_mean_ssp585.coords['time']\n",
    "y = total_volume_mean_ssp585\n",
    "hv.Curve((x, y),\n",
    "         kdims=x.name,\n",
    "         vdims=y.name,\n",
    "        ).opts(xlabel=x.attrs['description'],\n",
    "               ylabel=f\"{y.attrs['description']} in {y.attrs['unit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e3a3f",
   "metadata": {},
   "source": [
    "We used a [HoloViews Curve](https://holoviews.org/reference/elements/bokeh/Curve.html) and defined ```kdims``` and ```vdims```. This definition is not so important for a single plot but if we start to compose different plots all axis of the different plots with the same ```kdims``` or ```vdims``` are connected. This means for example whenever you zoom in on one plot all other plots also zoom in. Further, you can see that we have defined ```xlabel``` and ```ylabel``` using the variable description of the dataset, therefore it was useful to ```keep_attrs=True``` when we calculated the total values (see above).\n",
    "\n",
    "As a next step we can add the std as a shaded area ([HoloViews Area](https://holoviews.org/reference/elements/bokeh/Area.html)) and again define the whole plot as a single curve in a new function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd65edd",
   "metadata": {},
   "source": [
    "### Create single mean curve with std area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8f160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_single_curve(mean, std, ssp):\n",
    "    \n",
    "    mean_use = mean.loc[{'SCENARIO': ssp}]  # read out the mean of the SSP to plot \n",
    "    std_use = std.loc[{'SCENARIO': ssp}]  # read out the std of the SSP to plot\n",
    "    time = mean.coords['time']  # get the time for the x axis\n",
    "    \n",
    "    return (hv.Area((time,  # plot std as an area\n",
    "                     mean_use + std_use,  # upper boundary of the area\n",
    "                     mean_use - std_use),  # lower boundary of the area\n",
    "                    vdims=[mean_use.name, 'y2'],  # vdims for both boundaries\n",
    "                    kdims='time',\n",
    "                    label=ssp,\n",
    "                   ).opts(alpha=0.2,\n",
    "                          line_color=None,\n",
    "                         ) *\n",
    "            hv.Curve((time, mean_use),\n",
    "                     vdims=mean_use.name,\n",
    "                     kdims='time',\n",
    "                     label=ssp,\n",
    "                    )\n",
    "           ).opts(width=400,  # width of the total plot\n",
    "                  height=400,  # height of the total plot\n",
    "                  xlabel=time.attrs['description'],\n",
    "                  ylabel=f\"{mean_use.attrs['description']} in {mean_use.attrs['unit']}\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb58fb",
   "metadata": {},
   "source": [
    "### Overlay different scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba3353",
   "metadata": {},
   "source": [
    "The single curves of the different scenarios we can put together in a [HoloViews HoloMap](https://holoviews.org/reference/containers/bokeh/HoloMap.html), which is comparable to a dictionary. This further can be easily used to create a nice overlay of all curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172e3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overlay_scenarios(ds, variable):\n",
    "    hmap = hv.HoloMap(kdims='Scenarios')   # create a HoloMap\n",
    "    mean, std = calculate_total_mean_and_std(ds, variable)  # calculate mean and std for all SSPs using our previously defined function\n",
    "    for ssp in all_scenario:\n",
    "        hmap[ssp] = get_single_curve(mean, std, ssp)  # add a curve for each SSP to the HoloMap, using the RCP as a key (when you compare it do a dictonary)\n",
    "    return hmap.overlay().opts(title=variable)  # create an overlay of all curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef819c",
   "metadata": {},
   "source": [
    "### Show different variables in one figure and save as html file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f4b26",
   "metadata": {},
   "source": [
    "Now that we have defined how our plots should look like we can compose different variables we want to explore in one plot. To do so we can use Panel [Column](https://panel.holoviz.org/reference/layouts/Column.html) and [Row](https://panel.holoviz.org/reference/layouts/Row.html) for customization of the plot layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f7942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_plots = pn.Column(pn.Row(overlay_scenarios(ds_merged, 'volume'),\n",
    "                             overlay_scenarios(ds_merged, 'area'),\n",
    "                            ),\n",
    "                      overlay_scenarios(ds_merged, 'melt_on_glacier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06edfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3c2cc",
   "metadata": {},
   "source": [
    "When you start exploring the plots by dragging them around or zooming in (using the tools of the toolboxes in the upper right corner of each plot) you see that the x-axes are connected. This makes it very convenient for example to look at different periods for all variables interactively.\n",
    "\n",
    "You also can open the plots in a new browser tab by using\n",
    "\n",
    "```python\n",
    "all_plots.show()\n",
    "```\n",
    "\n",
    "or save it as a html file for sharing\n",
    "\n",
    "```python\n",
    "plots_to_save = pn.panel(all_plots)\n",
    "plots_to_save.save('GCM_runs.html', embed=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248814d",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "- return to the [OGGM documentation](https://docs.oggm.org)\n",
    "- back to the [table of contents](welcome.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
